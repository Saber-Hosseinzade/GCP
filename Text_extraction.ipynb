{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text extraction from image\n",
    "\n",
    "In this notebook, Python script is written to process each of the image files by sending them to the Google Vision API to identify the text in the image. The text from each image saved on Cloud Storage. If the text locale is not English (locale='en'), the text is sent to the Google Translate API to get an English translation for the original text. At the end, the script save the results in a BigQuery table. this project is written for [Integrate with Machine Learning APIs: Challenge Lab](https://run.qwiklabs.com/focuses/12704?parent=catalog)\n",
    "\n",
    "This diagram outlines the process:\n",
    "\n",
    "![title](images/project_diagram.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Configuring service account and credential file\n",
    "\n",
    "run below codes in cloud shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export credit=account           \n",
    "gcloud iam service-accounts create $credit\n",
    "gcloud projects add-iam-policy-binding $DEVSHELL_PROJECT_ID --member=serviceAccount:$credit@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com --role=roles/bigquery.admin\n",
    "gcloud projects add-iam-policy-binding $DEVSHELL_PROJECT_ID --member=serviceAccount:$credit@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com --role=roles/storage.admin\n",
    "gcloud iam service-accounts keys create OCR-key.json --iam-account $credit@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com\n",
    "export GOOGLE_APPLICATION_CREDENTIALS=${PWD}/OCR-key.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write python script in analyze-images.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: image_classification_dataset\n",
    "# Table name: image_text_detail\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import Google Cloud Library modules\n",
    "from google.cloud import storage, bigquery, language, vision, translate_v2\n",
    "\n",
    "project_name = sys.argv[1]\n",
    "bucket_name = sys.argv[2]\n",
    "\n",
    "# Set up our GCS, BigQuery, and Natural Language clients\n",
    "storage_client = storage.Client()\n",
    "bq_client = bigquery.Client(project=project_name)\n",
    "nl_client = language.LanguageServiceClient()\n",
    "\n",
    "# Set up client objects for the vision and translate_v2 API Libraries\n",
    "vision_client = vision.ImageAnnotatorClient()\n",
    "translate_client = translate_v2.Client()\n",
    "\n",
    "# Setup the BigQuery dataset and table objects\n",
    "dataset_ref = bq_client.dataset('image_classification_dataset')\n",
    "dataset = bigquery.Dataset(dataset_ref)\n",
    "table_ref = dataset.table('image_text_detail')\n",
    "table = bq_client.get_table(table_ref)\n",
    "\n",
    "# Create an array to store results data to be inserted into the BigQuery table\n",
    "rows_for_bq = []\n",
    "\n",
    "# Get a list of the files in the Cloud Storage Bucket\n",
    "files = storage_client.bucket(bucket_name).list_blobs()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "print('Processing image files from GCS. This will take a few minutes..')\n",
    "\n",
    "# Process files from Cloud Storage and save the result to send to BigQuery\n",
    "for file in files:    \n",
    "    if file.name.endswith('jpg') or  file.name.endswith('png'):\n",
    "        file_content = file.download_as_string()\n",
    "        \n",
    "        # TBD: Create a Vision API image object called image_object \n",
    "        from google.cloud import vision_v1\n",
    "        image_object = vision_v1.types.Image(content=file_content)\n",
    "\n",
    "\n",
    "        # TBD: Detect text in the image and save the response data into an object called response\n",
    "        response = vision_client.text_detection(image=image_object)\n",
    "    \n",
    "        # Save the text content found by the vision API into a variable called text_data\n",
    "        text_data = response.text_annotations[0].description\n",
    "\n",
    "        # Save the text detection response data in <filename>.txt to cloud storage\n",
    "        file_name = file.name.split('.')[0] + '.txt'\n",
    "        blob = bucket.blob(file_name)\n",
    "        \n",
    "        # Upload the contents of the text_data string variable to the Cloud Storage file \n",
    "        blob.upload_from_string(text_data, content_type='text/plain')\n",
    "\n",
    "        # Extract the description and locale data from the response file\n",
    "        # into variables called desc and locale\n",
    "        # using response object properties e.g. response.text_annotations[0].description\n",
    "        desc = response.text_annotations[0].description\n",
    "        locale = response.text_annotations[0].locale\n",
    "        \n",
    "        # if the locale is English (en) save the description as the translated_txt\n",
    "        if locale == 'en':\n",
    "            translated_text = desc\n",
    "        else:\n",
    "            # TBD: For non EN locales pass the description data to the translation API\n",
    "            # Set the target_language locale to 'en')\n",
    "            from google.cloud import translate_v2\n",
    "            \n",
    "            translate_client = translate_v2.Client()\n",
    "            translation = translate_client.translate(text_data, target_language='en')\n",
    "            translated_text = translation['translatedText']\n",
    "        print(translated_text)\n",
    "        \n",
    "        # if there is response data save the original text read from the image, \n",
    "        # the locale, translated text, and filename\n",
    "        if len(response.text_annotations) > 0:\n",
    "            rows_for_bq.append((desc, locale, translated_text, file.name))\n",
    "\n",
    "print('Writing Vision API image data to BigQuery...')\n",
    "\n",
    "# Write original text, locale and translated text to BQ\n",
    "errors = bq_client.insert_rows(table, rows_for_bq)\n",
    "\n",
    "assert errors == []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run python script\n",
    "\n",
    "Run this command in cloud shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 analyze-images.py $DEVSHELL_PROJECT_ID $DEVSHELL_PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run BigQuery\n",
    "\n",
    "Run this SQL command in BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT locale,COUNT(locale) as lcount FROM image_classification_dataset.image_text_detail GROUP BY locale ORDER BY lcount DESC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
